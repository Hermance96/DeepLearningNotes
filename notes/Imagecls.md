# 图像分类模型
## 模型综述
- 传统图像分类
    -主要分为两个流程
        - 特征提取，无监督学习
        - 分类，SVM, LR, RF, DT 等
    - 问题在于
        - 特征提取调整起来比较复杂，可能需要提取多种特征然后再进行组合，得到更好的特征；
        - 不符合人类认识事物的过程，并不会单独建立一个特征提取器
- 多层感知机
    - 由多层神经元密集连接形成的神经网络
    - 问题在于，如果样本数不足，很容易出现过拟合的问题
- CNN
    - 卷积层参数共享 -> 权重共享
    - 某一层的神经元只和上一层的部分神经元相关，而不是全部 -> 稀疏连接
    - 重点关注模型的以下参数
        - 准确率
        - 计算量
        - 模型的泛化能力
- 轻量级 CNN
    - 在一些应用场景中（移动/嵌入式设备），要求模型必须小而高效；
    - 目前主要有两个方向
        - 压缩复杂模型
        - 改进模型结构，设计并训练小模型
    - 目的为：尽量保持模型性能，降低模型大小，提升模型速度。

## AlexNet
- 较早提出
- 网络结构：卷积层x5 + 全连接层x3
- 采用 ReLU 激活函数
- 全连接层后面都加上一层 Dropout，减轻模型的过拟合问题

## VGG16
- 网络结构：卷积层x13 + 全连接层x3 + 池化层x5 （16指的是共16个权重层）
- 卷积层均采取相同的卷积核参数，池化层也都采取相同的池化核参数，所以结构深而简单
- 用连续的几个 3x3 卷积核，代替 AlexNet 中的大卷积核，这种堆积的小卷积核模式一般优于大卷积核，因为多层非线性通过增加网络深度，可以学习更加复杂的模式，而且参数也更少
- 存在的问题是，VGG 在内存和时间的要求上较高，计算量大，并不算高效

## Inception
- 提升网络性能 -> 增加网络深度和宽度 -> 易产生过拟合，且计算量大大增加
    - 采取稀疏连接
    - 但是非均匀的稀疏数据，其查找和缓存的开销很大
- GoogleLeNet 设计的 Inception 模块
    - 核心是，用密集连接来近似局部稀疏结构，稀疏矩阵 -> 相对密集的子矩阵
    - 同时使用不同大小的卷积核，然后拼接，融合不同尺度的特征
    - 采用大量的 1x1 瓶颈层（不改变宽高，只改变输出的通道数），降低通道数量，同时也引入更多的非线性
    - 最后用均值池化层，代替了全连接层（占模型参数比例很大），大大降低了参数数量

## ResNet
- 网络深度增加，有利于准确率的提升，但是也带来一些问题
    - 梯度消失，后向传播到比较靠前的层，梯度会很小，基本上学习停滞了
    - 参数过多，导致优化变得困难
    - 退化问题，损失趋于饱和后继续增加网络深度，损失反而会增大
- ResNet 的残差块解决深层网络的训练问题
    - 基本结构参考 VGG，增加短路连接 shortcut
    - 分为直接映射部分和残差部分（2、3个卷积操作）
    - 残差块表示为 H(x) = x + F(x)，即 F(x) 是预测值 H(x) 与单位映射的观测值 x 之差，所以称为残差
- 同样采取全局均值池化层
- 可以训练 152 层的残差网络，准确率和效率都比VGG更高

## DenseNet
- 基本思路与 ResNet 一致，但是连接更加密集，也是名字的由来
    - 互相连接所有层，也就是其中一层会接受前面所有层作为额外输入
    - 对于一个L层网络，共有 L(L+1)/2 个连接
    - 在channel维度上，直接拼接来自不同层的特征图，实现特征重用，提升效率
- 网络结构
    - Dense Block，包含的所有层特征图大小相同，层与层之间采取密集连接
        - 包含一个重要的超参数，在一个密集块中，卷积层的卷积核个数都为 k，称为 growth rate，一般采取较小的k性能更好
        - 对于后面输入的通道数过大，可以在中间增加瓶颈层，来降低维度
    - Transition，连接两个密集块，通过池化操作降低特征图大小，压缩模型

## MobileNet
- 基本单元：深度级可分离卷积 depthwise separable convolution
    - depthwise convolution (DW)，针对每个输入频道，采取不同的二维卷积核；
    - pointwise convolution (PW)，普通的卷积，采取 1x1 卷积核；
    - DWS 卷积，其实就是先采取 DW 对不同的输入频道分别卷积，然后再采取 PW，将各频道的输入结合；
    - 整体效果和一个标准的卷积类似，但是大大减少了参数的数量。
- DWS 举例
    - 假设输入维度 (11, 11, 3)，普通卷积核为 (3, 3, 3) x 16，步长取2，填充取1，那么最后输出维度为 (6, 6, 16)
    - 现在采取 DWS
        - 输入维度 (11, 11, 3)
        - DW kernel (3, 3) x 3，步长和填充不变，得到输出 (6, 6, 3)
        - PW kernel (1, 1, 3) x 16
        - 最后输出 (6, 6, 16)
- DWS 单元结构
    - 3x3 DW_conv
    - BN
    - ReLU
    - 1x1 PW_conv
    - BN
    - ReLU
- 再压缩，引入超参数：
    - width multiplier，按比例降低通道数
    - resolution multiplier，按比例缩小特征图的宽高（只降低计算量，不影响参数量）

### v2
- 采用的主要结构不变，仍然是 DWS_Conv
- 引入瓶颈层 bottleneck layer（线性激活），降低特征图的深度，提取”主要流形“
- Inverted Residual，在瓶颈层中间加上短路连接（降低内存使用？）
- bottleneck residual block
    - input layer
    - 1x1 expansion layer + BN + ReLU -> 解压器 
    - 3x3 DW Conv + BN + ReLU   -> 过滤器
    - 1x1 projection layer + BN -> 压缩器
    - 最后 projection layer 的输出加上input -> output

## ShuffleNet
- 核心操作
    - pointwise group convlution (PWG)，对输入的特征图先按通道分组，然后组内应用同一个卷积核；
        - 属于稀疏通道连接方式，和应用于所有特征图的全通道连接相比，降低了参数数量；
        - MobileNet 采取的 PW 其实就是每个分组只有1个特征图的特殊情况；
    - channel shuffle (CS)，均匀打乱通道，然后重组
        - 为了解决 GConv 中，不同分组之间没有通信的问题；
        - 以及避免采取类似 MobileNet 中参数量较大的PW方法实现跨组通信。
- 基本单元结构，在残差网络的基础上进行一些改进
    - input layer
    - 1x1 GConv
    - BN, ReLU
    - Channel Shuffle
    - 3x3 DW_conv
    - BN
    - 1x1 GConv
    - BN
    - add input
    - ReLU
    - output layer

### v2
- 引入新运算 channel split
    - 在通道维度，先将输入特征图分成两个分支（比如各一半）
    - 然后左分支做同等映射，右分支做卷积
    - 最后将两个分支的输出拼接在一起，进行 channel shuffle
- 从一定程度上来说，是对 DenseNet 的借鉴，在短路连接中用 concat 代替了 add 操作，实现了特征重用

## SqueezeNet
- 结构特点
    - 大量使用 1x1 卷积核，代替 3x3 的，降低参数数量；
    - 减少 3x3 卷积核的输入通道数
    - 延迟下采样，前面的层特征图较大，有利于提升准确率，后面层通过增大卷积层步长/池化，实现下采样，降低特征图尺寸
- 基本单元 Fire Module
    - 包含使用 1x1 卷积的 squeeze layer，以及混合使用 1x1 和 3x3 卷积的 expand layer
    - 所有卷积层的激活采取 ReLU，后面添加了随机失活 Dropout
    - 整个网络基本都由 FireModule 堆积形成
- 可引入一些短路连接，同样采取全局均值池化，代替全连接层