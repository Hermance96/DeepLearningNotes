# Keras笔记

* Keras是一个模型级的库，不负责处理张量操作、求微分等运算，交给张量库，也就是Keras的后端引擎 backend engine 来处理。
* 采用 Tensorflow 作为 Keras 的后端引擎，它应用广泛，可扩展性好，可用于生产环境。
* 安装的版本号为
    * tensorflow@1.13.1
    * tensorflow-base@1.13.1
    * tensorflow-estimator@1.13.0
    * keras@2.2.4
    * keras-base@2.2.4
    * keras-applications@1.0.8
    * keras-preprocessing@1.1.0

## 基础知识
### 张量
    张量 tensor 是一种数据容器，矩阵是二阶张量，张量是矩阵向任意维度的推广。
* 关键属性：
    * 阶，轴的个数，查看属性 ndim
    * 形状，张量沿每个轴的维度大小（元素个数），查看属性 shape
    * 数据类型，查看属性 dtype
* 张量切片 slice，沿着某个轴截取 [startIdx:endIdx]，表示从该轴的 startIdx 到 endIdx-1 的部分。如果采取了负数索引，就表示到该轴终点的相对位置。
* 张量运算
    * 广播 broadcast，高阶张量和低阶张量相加，会将低阶张量进行广播，添加新的广播轴，使其 ndim 和高阶的相同，然后沿着新轴不断重复，到形状和高阶的相同，最后逐元素计算。
    * 点积运算 tensor product，不是逐元素的乘积，调用方法 np.dot()
    * 张量变形，调用方法 reshape，变形后总元素的个数不变

### 算法描述
* 小批量随机梯度下降，即**小批量SGD**
    * 随机抽取样本x和对应y组成的数据批量
    * 前向传播，得到预测值y_pred
    * 计算损失函数
    * 后向传播，计算梯度
    * 根据梯度更新参数
* 每次迭代在所有数据上运行，称为**批量SGD**

### 计算流程
1. 加载数据集，以第一个例子，识别手写数字为例。
2. 设计网络架构
    * 层layer，是模型的组成构建，从输入数据中提取表示，通过链接多个层，实现渐进式的数据蒸馏 data distillation
    * 在本例中，模型包含2个全连接层FC，第二层是一个10路softmax分类层，返回图像属于各个数字的概率
3. 配置学习过程
    * 损失函数 loss function，训练过程中需要将其最小化，为学习提供反馈信号
    * 优化器 optimizer，决定基于损失函数，如何对网络参数进行更新
    训练和测试过程中需要监控的指标 metrics
4. 调整数据格式
    * 调整输入数据维度和数据格式，准备输出的标签
5. 训练模型
    * 调用 fit() 方法，指定 epochs 和 batch_size
    * 模型不会同时处理整个数据集，而是会拆分成小批量处理，这里批量的尺寸就是 batch_size
6. 评估模型
    * 最后用 evaluate() 方法评估模型在测试集上的表现

### 举例
1. 影评中的感情（正面/负面）——二分类问题
* 加载keras的IMDB数据集，然后进行张量化处理
    * train_data是一个25000维度的一阶向量，采用onehot编码，因为向量的数据范围都在0到9999之间，所以将每一个数扩展为一个长度为10000的数组，除了该数所在的位置取1，其余位置都填充0
    * 处理完之后，训练集为，X (25000, 10000), Y(25000, 1)
* 构建网络
    * 对于输入向量，输出{0, 1}的情况，采用relu激活的全连接层FC的简单堆叠，称之为 Dense，在这种问题上表现很好。
    * 本例中，设计的网络结构是，两层FC，均包含16个单元，采取relu激活函数；最后输出层只有1个单元，采取sigmoid激活函数。
    * 二分分类，最好采取二元交叉熵 binary_crossentropy 作为损失函数
    * RMSprop 是一个常见的优化器
* 方法验证
    * 将测试集留出一部分作为验证集，观察模型的表现；
    * 通过model.fit()训练模型，返回一个History对象，对象成员history是一个字典，包含训练过程中所有的数据。这里键值包含loss, acc, val_loss, val_acc，分别是训练集的损失、准确率，还有验证集的损失、准确率。
    * 从下图的对比可以看出，在 epochs=4 左右，就可以停下来了，否则会出现针对于训练集过拟合的情况。

<img src="imgs/movie1.png" width="800"> 

* 模型评估
    * 重新训练模型，设置训练轮数为4，调用evaluate()评估模型在测试集上的表现，准确率达到 87.4%

2. 新闻分类——多分类问题
* 加载keras中的reuters数据集，进行张量化处理后：
    * train_X (8982, 10000)
    * train_Y (8982, 46)
    * test_X (2246, 10000)
    * test_Y (2246, 46)
    * 输出结果为分别属于46个类别的概率，概率之和为1
* 构建网络
    * 本例中，设计的网络结构是，两层FC，均包含64个单元，采取relu激活函数；最后输出层有46个单元，采取 softmax 激活函数。
    * 中间隐藏层的单元数必须大于输出的单元数，否则就会产生信息瓶颈，相当于将信息压缩到维度很小的中间空间，必然会导致模型表现下降。
    * 多分类采取的损失函数为，多类交叉熵 categorical_crossentropy
    * 优化器依然采用 RMSprop
* 方法验证
    * 留出验证集。经验证，经过9轮左右的训练，就会发生过拟合。
* 模型评估
    * 最后在测试集上的表现为，准确率 77.6%

3. 波士顿房价预测——回归问题
* 加载keras中的boston_housing房价数据集：
    * train_X (404, 13)
    * test_Y (102, 13)
    * 然后进行归一化处理，求出训练集的平均值和标准差，将训练集和测试集的数据都减去平均值，再除以标准差。
* 构建网络
    * 本例中，设计的网络结构是，两层FC，均包含64个单元，采取relu激活函数；最后输出层只有1个单元，无激活函数。
    * 损失函数为，均方误差 mse，预测值和真实值误差的平方。
    * 评估标准为，平均绝对误差 mae，预测值和真实值误差的绝对值。
    * 优化器依然采用 RMSprop
* 方法验证
    * 设计K折交叉验证。每次取1/k的数据作为验证集，训练集中剩下的数据作为训练集；
    * 每次都新建模型，然后调用fit()，进行训练，记录下在当前验证集上的表现；
    * 根据验证集的表现，调整训练轮数，避免训练集过拟合。
* 模型评估
    * 最后选择训练轮数为60左右，这是在验证集上表现最好的参数。
    * 在测试集上的表现为，mae=2.64

### 机器学习通用流程
1. 定义问题，收集数据集。
    * 明确输入、输出，和可用数据，假设可用数据包含足够多的信息，可以用于学习输入和输出之间的关系。
    * 注意到，机器学习是基于训练数据中存在的过去的数据，来预测未来，所以未来和过去规律相同时效果更好。
2. 选择衡量指标。
    * 精度，准确率，召回率？
    * 根据业务目标设置衡量指标。
3. 确定评估方法。
    * 留出验证 holdout validation，将训练集数据留出一部分不参与训练，作为验证集；
    * K折验证 k-fold validation，将数据划分为大小相同的K个分区，每次取其中一个作为验证集，进行K次；
    * 重复K折验证 k-fold validation with shuffling，每次划分K个分区之前，都对数据进行打乱，进行P*K次。
4. 准备数据。
    * 格式化为张量；
    * 数据标准化处理；
    * 特征工程。
5. 开发比基准更好的模型。
    * 获得统计功效 statistical power，开发的模型可以打败纯随机基准，比如二分类问题中，acc高于0.5；
    * 配置模型，选择激活函数、损失函数、优化器等。
6. 扩大模型规模，开发过拟合的模型。
    * 添加更多的层，让每一层变得更大，或者训练更多的轮次，直到出现过拟合。
7. 模型正则化与调节超参数。
    * 不断调节模型、训练、在验证集上评估、再次调节模型，重复直到模型达到最佳性能。
    * 正则化：L1 和/或 L2 正则化，或是使用 dropout。
    * 尝试不同的架构：增加或减少层数。
    * 尝试不同的超参数组合，找到最佳配置。
    * 反复做特征工程，添加新特征或删除没有信息量的特征。

## 卷积神经网络
1. 简单CNN —— MNIST数字识别问题

<img src="imgs/k_conv1.png" width="500">

* 这是比较典型的结构，前面采取卷积层+池化层重复的形式，这一部分称为模型的卷积基 convolutional base，然后进行展开，接着几个全连接层，最后是输出分类结果的输出层，这一部分是模型的分类器。
* 在keras中
    * 卷积层，layers.Conv
    * 池化层（最大），layers.MaxPooling
    * 全连接层，layers.Dense
    * 展开层，layers.Flatten
* 卷积层和池化层，需要指定过滤器的尺寸；除了展开层，都需要指定本层所使用的激活函数；
* 最后对于mnist，模型在测试集上的acc=99.11%

2. 数据增强&dropout —— 识别猫狗问题
* 下载Kaggle/dogs-vs-cats问题的数据，取1000张猫的照片作为训练集，500张验证集，500张测试集，狗的照片也一样。
* 建立模型，采取结构为：
    * Conv2D(32, f=3) & MaxPooling2D(f=2)
    * Conv2D(64, f=3) & MaxPooling2D(f=2)
    * Conv2D(128, f=3) & MaxPooling2D(f=2)
    * Conv2D(128, f=3) & MaxPooling2D(f=2)
    * Flatten()
    * Dense(512)
    * 输出层，Dense(1)
* 数据处理
    * 图片格式为jpg，处理的时候有以下步骤：读取图像文件，将文件解码为 RGB 像素网格，将这些像素网格转换为浮点数张量，最后再将像素值(0~255 范围内)缩放到 [0, 1] 区间。
    * 使用 Keras 的 ImageDataGenerator 类，通过生成器进行图片的处理：
        * 调用 flow_from_directory() 方法
        * 指定读取目录，图像目标大小，处理批大小，标签格式等。
* 模型训练
    * 调用 fit_generator() 方法；
    * 指定生成器，以及每轮从生成器中抽取的样本数量 steps_per_epoch，训练轮数 epochs，验证集的生成器和每次抽取的样本数量 validation_steps
* 模型评估
    * 基本上5轮以后就出现了过拟合；

<img src="imgs/cvd1.png" width="600">

* 数据增强 & dropout 解决过拟合
    * ImageDataGenerator构造参数：
        * rotation_range，图像旋转的角度范围；
        * width_shift/height_shift，图像在水平/垂直方向平移范围；
        * shear_range，图像错切变换角度范围；
        * zoom_range，图像缩放范围；
        * horizontal_flip，水平翻转；
        * fill_mode，填充新创建像素的方法。
    * 对训练集的数据，进行数据增强；
    * 改变模型结构，在展开 Flatten 后增加：
        * layers.Dropout(0.5)
* 再次评估模型
    * 过拟合有了明显改善，acc在训练集和验证集上都保持上涨趋势，训练轮数目前采取的是80，还可以再增加。

<img src="imgs/cvd2.png" width="600">

3. 迁移学习
* 使用预训练模型（迁移学习），分为两种：
    * 特征提取 feature extraction，复用模型的卷积基。因为卷积基部分学习到的东西更加通用，FC分类器舍弃了空间的概念，学习到的只是某类物体出现在图像中的概率信息。
    * 模型微调 fine-tuning，将卷积基的顶部几层解冻，调节解冻的部分和分类器。因为模型的底部层提取的是高度通用的特征，比如边缘、颜色、纹理等，所以复用时保留底部，只对顶部几层解冻即可。
* 从 keras.applications 中，引入模型 VGG16
    * weights 指定模型初始化的权重检查点
    * include_top，迁移学习中一般选择False，不包含分类器
* 特征提取有两种使用方法：
    * 在数据集上运行卷积基，保存结果为 Numpy 数组，将这个数组作为独立的分类器的输入。优点是速度快，计算开销小，因为只运行了一次卷积基，但是不允许使用数据增强；
    * 在模型顶部添加Dense层，扩展已有模型。可以使用数据增强，但是计算代价高很多。
        * model包括，conv_base, Flatten, Dense(256), Dense(1)
        * 注意需要先设置 conv_base.trainable=False，冻结卷积基的参数；
        * 然后使用数据增强，设置生成器；
        * 最后编译模型，针对生成器进行训练 fit_generator()
    * 模型微调的过程和上面类似，但是要指定卷积基中解冻层，设置好各层的 trainable 属性。
* 这里只对第一种计算开销小的方法进行实验：
    * 保存卷积基训练结果，reshape展开输入的特征；
    * 建立分类器模型：FC(256) + Dropout(0.5) + FC(1)
    * 模型编译，训练（展开的特征作为训练输入）
* 观察结果发现，基本上5轮以后就出现了过拟合，但是到第5轮，验证acc就已经达到了90%以上，相比前面自己建立的模型，还是有很大提升。
* 对于数据量小的数据集，采取数据增强是解决过拟合问题的重要手段。

<img src="imgs/cvd3.png" width="600">

4. 可视化
* 可视化中间激活
    * 展示1张小猫图片，经过模型前8层处理后，每层的激活输出图像；
        * 处理图片为张量 img_tensor
        * 提取前8层的输入和输出，实例化新的Model
        * 调用 predict() 获得图片张量对应的输出 activations
        * 对于每一层的 activation，输出的一张图片为 (0, :, :, channel)，按频道号依次排列输出。
    * 第一层是各种边缘探测器的集合，这一层的激活几乎保留了原始图像中的所有信息；
    * 随着层数的加深，激活变得越来越抽象。层数越深，其中关于图像视觉内容的信息就越少，关于类别的信息就越多；
    * 激活的稀疏度 sparsity 随着层数的加深而增大；
    * 深度神经网络可以作为有效的信息蒸馏通道 information distillation pipeline，输入原始数据，反复过滤，放大和细化有效信息。
* 可视化过滤器
    * 底层的过滤器对应简单的方向、边缘和颜色；
    * 随着层数加深，出现边缘和颜色组合而成的简单纹理。
    * 再高层的过滤器纹理更加复杂，类似于自然图像中的纹理。
* 可视化类激活图 CAM
    * CAM生成热力图，表示图像每个位置对该类别的重要程度；
    * 应用方法，GradCAM
        * 给定一张输入图像，对于一个卷积层的输出特征图，用类别相对于通道的梯度对这个特征图中的每个通道进行加权。
        * 相当于，用 “每个通道对类别的重要程度” 对 “输入图像对不同通道的激活强度” 的空间图进行加权，从而得到了 “输入图像对类别的激活强度” 的空间图。
    * 通过叠加原始图和热力图，可以看出模型识别出物体的具体位置和判别的重要依据。

## 文本处理
* 分割文本为 token，这个过程是分词 Tokenization，将文本分割成单词/字符/多个单词组成的词袋 n-gram
* 编码方法
    * one-hot，每个单词与一个整数索引相映射，单词表共包含N个单词，单词转换为一个长度为N的向量，只在其索引处取1，其他均为0；
    * 词嵌入 word embedding
        * one-hot编码的结果是二进制、稀疏的，维度很高，而词嵌入得到的是低维的浮点数向量；
        * 方法有两种：随机化初始词向量，在完成主任务的同时学习词嵌入；或是，预计算词嵌入，再将其加载到模型中。
        * 词嵌入将语言映射到空间中，同义词应当距离较近，特定的方向也能代表某些含义，是可以解释的、有实际意义的，比如说king向量+female向量，应该得到queen向量。目前不存在完美的嵌入空间，词嵌入空间的选择主要看需要完成的学习任务是什么。
        * keras.layers.Embedding，Embedding层将表示特定单词的单词索引，映射为密集向量。实际是一种字典查找，接收整数，在内部字典中查找这些整数

### LSTM

<img src="imgs/lstm1.png" width="400">

* 模型原理
    * LSTM 对细胞状态中的信息进行遗忘和记忆，使得对后续时刻计算有用的信息得以传递，而丢弃无用的信息；
    * 根据前一时刻隐层状态 h(t-1), 输入 Xt, 计算遗忘门 ft
    * 根据前一时刻隐层状态 h(t-1), 输入 Xt, 计算记忆门 it
    * 根据前一时刻隐层状态 h(t-1), 输入 Xt, 计算输出门 ot
    * 记忆门和临时细胞状态，计算出当前细胞状态
    * 当前细胞状态和输出门，计算出下一个时刻的隐层状态 ht
* Bi-LSTM，是前向LSTM和后向LSTM的结合，将文本分别前向输入和后向输入，得到的向量进行拼接。